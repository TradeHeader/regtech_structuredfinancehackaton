{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports, setting environment and GPT API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv #library to read the environment variables\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader #there are a bunch of loaders in langchain library\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from operator import itemgetter\n",
    "\n",
    "#environment variables\n",
    "load_dotenv()\n",
    "\n",
    "#key config\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") #get the key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to generate the model and the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_generator(MODEL, document=None):\n",
    "    \"\"\"\n",
    "    Generates a LangChain processing chain for answering questions based on the context extracted from a document.\n",
    "\n",
    "    Parameters:\n",
    "    - MODEL (str): The name of the AI model to use. This can be a GPT model (e.g., \"gpt-3.5-turbo\") from OpenAI or an Ollama model.\n",
    "    - document (str): The path to the PDF document that will be used as the context source for answering questions.\n",
    "\n",
    "    Returns:\n",
    "    - chain (Chain): A LangChain object representing the processing chain. This chain includes context retrieval,\n",
    "      question handling, and the final model response based on the provided document context.\n",
    "    \"\"\"\n",
    "    #sets the model\n",
    "    if MODEL.startswith(\"gpt\"):\n",
    "        model = ChatOpenAI(api_key=OPENAI_API_KEY, model=MODEL)\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "    else:\n",
    "        model = Ollama(model=MODEL)\n",
    "        embeddings = OllamaEmbeddings()\n",
    "\n",
    "    print(\"Setting up parser and prompt template...\")\n",
    "    #parser\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    #prompt\n",
    "    template = \"\"\"\n",
    "    Objective: Given a CDM (Common Domain Model) TradeState, \n",
    "            determine which of the following regulatory bodies the transaction must be reported to:\n",
    "            - CFTC (Commodity Futures Trading Commission)\n",
    "            - EMIR (European Market Infrastructure Regulation)\n",
    "            - JFSA (Japan Financial Services Agency)\n",
    "            - UKEMIR (UK European Market Infrastructure Regulation)\n",
    "            - MAS (Monetary Authority of Singapore)\n",
    "            - ASIC (Australian Securities and Investments Commission)\n",
    "\n",
    "            Output:\n",
    "            Just enumerate the regulatory bodies which are applicable. Do not include any other word (e.g. \"CFTC, EMIR, JFSA\").\n",
    "            If no regulatory bodies apply for this trade transaction return \"None\".\n",
    "\n",
    "    CDM TradeState: {question}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(template) #takes the prompt\n",
    "\n",
    "    print(\"Building the final chain...\")\n",
    "    #final chain\n",
    "    chain = (\n",
    "        prompt\n",
    "        | model\n",
    "        | parser\n",
    "    )\n",
    "    print (\"Chain completed!\")\n",
    "    print (\"===========================================================\")\n",
    "    return chain\n",
    "\n",
    "def get_answers (chain, questions):\n",
    "    \"\"\"\n",
    "    Retrieves answers to a list of questions using a given LangChain processing chain.\n",
    "\n",
    "    Parameters:\n",
    "    - chain (Chain): A LangChain object representing the processing chain for retrieving and answering questions.\n",
    "    - questions (list of str): A list of questions to be answered using the provided chain.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are 'sample-1', 'sample-2', etc., and values are the corresponding answers.\n",
    "    \"\"\"\n",
    "    ans_dict = {}\n",
    "    for idx, question in enumerate(questions):\n",
    "        answer = chain.invoke({\"question\": question})\n",
    "        key = f\"sample-{idx + 1}\"\n",
    "        ans_dict[key] = answer\n",
    "    return ans_dict\n",
    "\n",
    "\n",
    "\n",
    "def eligibility_detector(sample_paths):\n",
    "    \"\"\"\n",
    "    Opens the files and apply the LLM to return the jurisdictions where the transaction needs to be reported.\n",
    "\n",
    "    Parameters:\n",
    "    - sample_paths: A string with a single relative path or a list of strings with the relative paths to the CDM samples representing the transactions.\n",
    "\n",
    "    Returns:\n",
    "     - dict: A dictionary where keys are 'sample-1', 'sample-2', etc., and values are the corresponding answers.\n",
    "    \"\"\"\n",
    "    # Single cardinality case\n",
    "    if type(sample_paths) == str: \n",
    "        sample_paths = [sample_paths]\n",
    "    # Open and read the JSON file content as a string\n",
    "    sample_json = []\n",
    "    for sample_path in sample_paths:\n",
    "        with open(str(sample_path), 'r') as file:\n",
    "            sample_json.append(file.read())\n",
    "    return get_answers (chain_generator(\"gpt-4o\"), sample_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up parser and prompt template...\n",
      "Building the final chain...\n",
      "Chain completed!\n",
      "===========================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sample-1': 'CFTC, EMIR, UKEMIR', 'sample-2': 'CFTC, EMIR'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multiple samples\n",
    "response_multiple = eligibility_detector(['cdm-samples/credit-index-abx-abxhe.json', 'cdm-samples/credit-indextranche-abx-abxtranche.json'])\n",
    "response_multiple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up parser and prompt template...\n",
      "Building the final chain...\n",
      "Chain completed!\n",
      "===========================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sample-1': 'CFTC'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#single sample\n",
    "response_single = eligibility_detector('cdm-samples/credit-index-abx-abxhe.json')\n",
    "response_single"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
